const char* QN_fltvec_bmul2_rcsid = "$Header: /u/drspeech/repos/quicknet2/QN_fltvec_bmul2.cc,v 1.2 2004/04/20 00:57:13 davidj Exp $";

// Floating point vector utility routines for QuickNet
// Bunch-mode floating point matrix ops

#include <assert.h>
#include "QN_fltvec.h"


/*
**
** Matrix-Matrix Code for the operation:
**    C = alpha*transpose(A)*B + C
**
** Automatically Generated by mm_gen ($Revision: 1.2 $) using the command:
**    /n/burrito/da/bilmes/phipac/mm_gen/mm_gen -cb 2 4 10 -cb 26 13 5 -file /n/marmite/db/davidj/tmp/dj -prec single -routine_name _php_multnacc_fmfmf_mf -alpha c -beta 1 -opA t -alpha c 
**
** Generated on: Sunday June 25 1995, 13:37:20 PDT
** Created by: Jeff Bilmes <bilmes@cs.berkeley.edu>
**
*/

#define ZERO1x1(c00) \
{\
   c00 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x1(c00,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; \
}

/* Fixed M,K,N = 1,1,1 matrix matrix multiply. */
#define mul_tmf1x1mf1x1_mf1x1(c00,A,Astride,B,Bstride) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,1 matrix matrix multiply. */
#define mul_tmf1x2mf2x1_mf1x1(c00,A,Astride,B,Bstride) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 1,4,1 matrix matrix multiply. */
#define mul_tmf1x4mf4x1_mf1x1(c00,A,Astride,B,Bstride) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   A += Astride; \
}


#define ZERO1x2(c00,c01) \
{\
   c00 = 0.0; c01 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x2(c00,c01,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; \
}

/* Fixed M,K,N = 1,1,2 matrix matrix multiply. */
#define mul_tmf1x1mf1x2_mf1x2(c00,c01,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,2 matrix matrix multiply. */
#define mul_tmf1x2mf2x2_mf1x2(c00,c01,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   A += Astride; \
}


/* Fixed M,K,N = 1,4,2 matrix matrix multiply. */
#define mul_tmf1x4mf4x2_mf1x2(c00,c01,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   A += Astride; \
}


#define ZERO1x4(c00,c01,c02,c03) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x4(c00,c01,c02,c03,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; \
}

/* Fixed M,K,N = 1,1,4 matrix matrix multiply. */
#define mul_tmf1x1mf1x4_mf1x4(c00,c01,c02,c03,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,4 matrix matrix multiply. */
#define mul_tmf1x2mf2x4_mf1x4(c00,c01,c02,c03,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   A += Astride; \
}


/* Fixed M,K,N = 1,4,4 matrix matrix multiply. */
#define mul_tmf1x4mf4x4_mf1x4(c00,c01,c02,c03,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   A += Astride; \
}


#define ZERO1x8(c00,c01,c02,c03,c04,c05,c06,c07) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; c04 = 0.0; c05 = 0.0; c06 = 0.0; c07 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x8(c00,c01,c02,c03,c04,c05,c06,c07,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; _cp[4] += alpha*c04; _cp[5] += alpha*c05; _cp[6] += alpha*c06; _cp[7] += alpha*c07; \
}

/* Fixed M,K,N = 1,1,8 matrix matrix multiply. */
#define mul_tmf1x1mf1x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,8 matrix matrix multiply. */
#define mul_tmf1x2mf2x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   A += Astride; \
}


/* Fixed M,K,N = 1,4,8 matrix matrix multiply. */
#define mul_tmf1x4mf4x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   A += Astride; \
}


#define ZERO1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; c04 = 0.0; c05 = 0.0; c06 = 0.0; c07 = 0.0; c08 = 0.0; c09 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; _cp[4] += alpha*c04; _cp[5] += alpha*c05; _cp[6] += alpha*c06; _cp[7] += alpha*c07; _cp[8] += alpha*c08; _cp[9] += alpha*c09; \
}

/* Fixed M,K,N = 1,1,10 matrix matrix multiply. */
#define mul_tmf1x1mf1x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   A += Astride; \
}


/* Fixed M,K,N = 1,2,10 matrix matrix multiply. */
#define mul_tmf1x2mf2x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   A += Astride; \
}


/* Fixed M,K,N = 1,4,10 matrix matrix multiply. */
#define mul_tmf1x4mf4x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   A += Astride; \
}


#define ZERO2x1(c00,c10) \
{\
   c00 = 0.0; \
   c10 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE2x1(c00,c10,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; \
}

/* Fixed M,K,N = 2,1,1 matrix matrix multiply. */
#define mul_tmf2x1mf1x1_mf2x1(c00,c10,A,Astride,B,Bstride) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   _a = A[1]; \
   c10 += _a*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 2,2,1 matrix matrix multiply. */
#define mul_tmf2x2mf2x1_mf2x1(c00,c10,A,Astride,B,Bstride) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   _a = A[1]; \
   c10 += _a*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   _a = A[1]; \
   c10 += _a*_b0; \
   A += Astride; \
}


/* Fixed M,K,N = 2,4,1 matrix matrix multiply. */
#define mul_tmf2x4mf4x1_mf2x1(c00,c10,A,Astride,B,Bstride) \
{ \
   register float _b0; \
   register float _a; \
   \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   _a = A[1]; \
   c10 += _a*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   _a = A[1]; \
   c10 += _a*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   _a = A[1]; \
   c10 += _a*_b0; \
   A += Astride; \
   \
   _b0 = B[0]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; \
   _a = A[1]; \
   c10 += _a*_b0; \
   A += Astride; \
}


#define ZERO2x2(c00,c01,c10,c11) \
{\
   c00 = 0.0; c01 = 0.0; \
   c10 = 0.0; c11 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE2x2(c00,c01,c10,c11,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; _cp[1] += alpha*c11; \
}

/* Fixed M,K,N = 2,1,2 matrix matrix multiply. */
#define mul_tmf2x1mf1x2_mf2x2(c00,c01,c10,c11,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   A += Astride; \
}


/* Fixed M,K,N = 2,2,2 matrix matrix multiply. */
#define mul_tmf2x2mf2x2_mf2x2(c00,c01,c10,c11,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   A += Astride; \
}


/* Fixed M,K,N = 2,4,2 matrix matrix multiply. */
#define mul_tmf2x4mf4x2_mf2x2(c00,c01,c10,c11,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; \
   A += Astride; \
}


#define ZERO2x4(c00,c01,c02,c03,c10,c11,c12,c13) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; \
   c10 = 0.0; c11 = 0.0; c12 = 0.0; c13 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE2x4(c00,c01,c02,c03,c10,c11,c12,c13,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; _cp[1] += alpha*c11; _cp[2] += alpha*c12; _cp[3] += alpha*c13; \
}

/* Fixed M,K,N = 2,1,4 matrix matrix multiply. */
#define mul_tmf2x1mf1x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   A += Astride; \
}


/* Fixed M,K,N = 2,2,4 matrix matrix multiply. */
#define mul_tmf2x2mf2x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   A += Astride; \
}


/* Fixed M,K,N = 2,4,4 matrix matrix multiply. */
#define mul_tmf2x4mf4x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; \
   A += Astride; \
}


#define ZERO2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; c04 = 0.0; c05 = 0.0; c06 = 0.0; c07 = 0.0; \
   c10 = 0.0; c11 = 0.0; c12 = 0.0; c13 = 0.0; c14 = 0.0; c15 = 0.0; c16 = 0.0; c17 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; _cp[4] += alpha*c04; _cp[5] += alpha*c05; _cp[6] += alpha*c06; _cp[7] += alpha*c07; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; _cp[1] += alpha*c11; _cp[2] += alpha*c12; _cp[3] += alpha*c13; _cp[4] += alpha*c14; _cp[5] += alpha*c15; _cp[6] += alpha*c16; _cp[7] += alpha*c17; \
}

/* Fixed M,K,N = 2,1,8 matrix matrix multiply. */
#define mul_tmf2x1mf1x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   A += Astride; \
}


/* Fixed M,K,N = 2,2,8 matrix matrix multiply. */
#define mul_tmf2x2mf2x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   A += Astride; \
}


/* Fixed M,K,N = 2,4,8 matrix matrix multiply. */
#define mul_tmf2x4mf4x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; \
   A += Astride; \
}


#define ZERO2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19) \
{\
   c00 = 0.0; c01 = 0.0; c02 = 0.0; c03 = 0.0; c04 = 0.0; c05 = 0.0; c06 = 0.0; c07 = 0.0; c08 = 0.0; c09 = 0.0; \
   c10 = 0.0; c11 = 0.0; c12 = 0.0; c13 = 0.0; c14 = 0.0; c15 = 0.0; c16 = 0.0; c17 = 0.0; c18 = 0.0; c19 = 0.0; \
}

#define SCALE_ACCUMULATE_STORE2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,C,Cstride) \
{\
   float *_cp = C; \
   _cp[0] += alpha*c00; _cp[1] += alpha*c01; _cp[2] += alpha*c02; _cp[3] += alpha*c03; _cp[4] += alpha*c04; _cp[5] += alpha*c05; _cp[6] += alpha*c06; _cp[7] += alpha*c07; _cp[8] += alpha*c08; _cp[9] += alpha*c09; \
   _cp += Cstride; \
   _cp[0] += alpha*c10; _cp[1] += alpha*c11; _cp[2] += alpha*c12; _cp[3] += alpha*c13; _cp[4] += alpha*c14; _cp[5] += alpha*c15; _cp[6] += alpha*c16; _cp[7] += alpha*c17; _cp[8] += alpha*c18; _cp[9] += alpha*c19; \
}

/* Fixed M,K,N = 2,1,10 matrix matrix multiply. */
#define mul_tmf2x1mf1x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   A += Astride; \
}


/* Fixed M,K,N = 2,2,10 matrix matrix multiply. */
#define mul_tmf2x2mf2x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   A += Astride; \
}


/* Fixed M,K,N = 2,4,10 matrix matrix multiply. */
#define mul_tmf2x4mf4x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,A,Astride,B,Bstride) \
{ \
   register float _b0,_b1,_b2,_b3,_b4,_b5,_b6,_b7,_b8,_b9; \
   register float _a; \
   \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   A += Astride; \
   \
   _b0 = B[0]; _b1 = B[1]; _b2 = B[2]; _b3 = B[3]; _b4 = B[4]; _b5 = B[5]; _b6 = B[6]; _b7 = B[7]; _b8 = B[8]; _b9 = B[9]; \
   B += Bstride; \
   _a = A[0]; \
   c00 += _a*_b0; c01 += _a*_b1; c02 += _a*_b2; c03 += _a*_b3; c04 += _a*_b4; c05 += _a*_b5; c06 += _a*_b6; c07 += _a*_b7; c08 += _a*_b8; c09 += _a*_b9; \
   _a = A[1]; \
   c10 += _a*_b0; c11 += _a*_b1; c12 += _a*_b2; c13 += _a*_b3; c14 += _a*_b4; c15 += _a*_b5; c16 += _a*_b6; c17 += _a*_b7; c18 += _a*_b8; c19 += _a*_b9; \
   A += Astride; \
}


/* Fixed M,N = 52,50, Arbitrary K L1-blocked matrix matrix multiply. */
void
_php_multnacc_fmfmf_mf_l1_arb_k(int K, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0;
   const float *bp0;
   float *cp0;
   const int C_sbs_stride = Cstride*2;
   const int k_marg_el = K & 3;
   const int k_norm = (K - k_marg_el)*Astride;
   float *const c0_endp = C+52*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=2) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + 50;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=10,cp0+=10) {
         ap0=a0;
         bp0=b0;
         ZERO2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19);
         for (; ap0!=ap0_endp; ) {
            mul_tmf2x4mf4x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf2x2mf2x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf2x1mf1x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
      }
   }
}

/* Arbitrary M,K,N L1-blocked matrix matrix multiply. */
void
_php_multnacc_fmfmf_mf_l1_arb_all(const int M, const int K, const int N, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0;
   const float *bp0;
   float *cp0;
   const int C_sbs_stride = Cstride*2;
   const int k_marg_el = K & 3;
   const int k_norm = (K - k_marg_el)*Astride;
   const int m_marg_el = M & 1;
   const int m_norm = M - m_marg_el;
   const int n_marg_el = N % 10;
   const int n_norm = N - n_marg_el;
   float *const c0_endp = C+m_norm*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=2) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=10,cp0+=10) {
         ap0=a0;
         bp0=b0;
         ZERO2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19);
         for (; ap0!=ap0_endp; ) {
            mul_tmf2x4mf4x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf2x2mf2x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf2x1mf1x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
      }
   }
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=2) {
      const float* const ap0_endp = a0 + k_norm;
      b0 = B+n_norm;
      cp0 = c0+n_norm;
      if (n_marg_el & 0x8) {
         ap0=a0;
         bp0=b0;
         ZERO2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17);
         for (; ap0!=ap0_endp; ) {
            mul_tmf2x4mf4x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf2x2mf2x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf2x1mf1x8_mf2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE2x8(c00,c01,c02,c03,c04,c05,c06,c07,c10,c11,c12,c13,c14,c15,c16,c17,cp0,Cstride);
         b0 += 8;
         cp0 += 8;
      }
      if (n_marg_el & 0x4) {
         ap0=a0;
         bp0=b0;
         ZERO2x4(c00,c01,c02,c03,c10,c11,c12,c13);
         for (; ap0!=ap0_endp; ) {
            mul_tmf2x4mf4x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf2x2mf2x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf2x1mf1x4_mf2x4(c00,c01,c02,c03,c10,c11,c12,c13,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE2x4(c00,c01,c02,c03,c10,c11,c12,c13,cp0,Cstride);
         b0 += 4;
         cp0 += 4;
      }
      if (n_marg_el & 0x2) {
         ap0=a0;
         bp0=b0;
         ZERO2x2(c00,c01,c10,c11);
         for (; ap0!=ap0_endp; ) {
            mul_tmf2x4mf4x2_mf2x2(c00,c01,c10,c11,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf2x2mf2x2_mf2x2(c00,c01,c10,c11,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf2x1mf1x2_mf2x2(c00,c01,c10,c11,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE2x2(c00,c01,c10,c11,cp0,Cstride);
         b0 += 2;
         cp0 += 2;
      }
      if (n_marg_el & 0x1) {
         ap0=a0;
         bp0=b0;
         ZERO2x1(c00,c10);
         for (; ap0!=ap0_endp; ) {
            mul_tmf2x4mf4x1_mf2x1(c00,c10,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf2x2mf2x1_mf2x1(c00,c10,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf2x1mf1x1_mf2x1(c00,c10,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE2x1(c00,c10,cp0,Cstride);
      }
   }
   if (m_marg_el & 0x1) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + n_norm;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=10,cp0+=10) {
         ap0=a0;
         bp0=b0;
         ZERO1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x4mf4x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf1x2mf2x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x10_mf1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,cp0,Cstride);
      }
      if (n_marg_el & 0x8) {
         ap0=a0;
         bp0=b0;
         ZERO1x8(c00,c01,c02,c03,c04,c05,c06,c07);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x4mf4x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf1x2mf2x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x8_mf1x8(c00,c01,c02,c03,c04,c05,c06,c07,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x8(c00,c01,c02,c03,c04,c05,c06,c07,cp0,Cstride);
         b0 += 8;
         cp0 += 8;
      }
      if (n_marg_el & 0x4) {
         ap0=a0;
         bp0=b0;
         ZERO1x4(c00,c01,c02,c03);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x4mf4x4_mf1x4(c00,c01,c02,c03,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf1x2mf2x4_mf1x4(c00,c01,c02,c03,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x4_mf1x4(c00,c01,c02,c03,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x4(c00,c01,c02,c03,cp0,Cstride);
         b0 += 4;
         cp0 += 4;
      }
      if (n_marg_el & 0x2) {
         ap0=a0;
         bp0=b0;
         ZERO1x2(c00,c01);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x4mf4x2_mf1x2(c00,c01,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf1x2mf2x2_mf1x2(c00,c01,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x2_mf1x2(c00,c01,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x2(c00,c01,cp0,Cstride);
         b0 += 2;
         cp0 += 2;
      }
      if (n_marg_el & 0x1) {
         ap0=a0;
         bp0=b0;
         ZERO1x1(c00);
         for (; ap0!=ap0_endp; ) {
            mul_tmf1x4mf4x1_mf1x1(c00,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x2) {
            mul_tmf1x2mf2x1_mf1x1(c00,ap0,Astride,bp0,Bstride);
         }
         if (k_marg_el & 0x1) {
            mul_tmf1x1mf1x1_mf1x1(c00,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE1x1(c00,cp0,Cstride);
      }
   }
}

/* Fixed M,K,N = 52,52,50 L1-blocked matrix matrix multiply. */
void
_php_multnacc_fmfmf_mf_l1(const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
{
   const float *a0,*b0;
   float *c0;
   const float *ap0;
   const float *bp0;
   float *cp0;
   const int C_sbs_stride = Cstride*2;
   const int k_norm = 52*Astride;
   float *const c0_endp = C+52*Cstride;
   register float c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19;
   for (c0=C,a0=A; c0!= c0_endp; c0+=C_sbs_stride,a0+=2) {
      const float* const ap0_endp = a0 + k_norm;
      float* const cp0_endp = c0 + 50;
      for (b0=B,cp0=c0; cp0!=cp0_endp; b0+=10,cp0+=10) {
         ap0=a0;
         bp0=b0;
         ZERO2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19);
         for (; ap0!=ap0_endp; ) {
            mul_tmf2x4mf4x10_mf2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,ap0,Astride,bp0,Bstride);
         }
         SCALE_ACCUMULATE_STORE2x10(c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,cp0,Cstride);
      }
   }
}

void
qn_pp_multnacc_fmfmf_mf_sss(const int M, const int K, const int N, const float *const A, const float *const B, float *const C, const int Astride, const int Bstride, const int Cstride, const float alpha)
{
   /* Code for L2-blocked routine. */
   int m2,k2,n2;
   const float *a2,*b2;
   float *c2;
   const float *ap2,*bp2;
   float *cp2;
   if (M < 53 && K < 53 && N < 51) {
      _php_multnacc_fmfmf_mf_l1_arb_all(M,K,N,A,B,C,Astride,Bstride,Cstride,alpha);
      return;
   }
   for (m2=0; m2<=M-52; m2+=52) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<=N-50; n2+=50,b2+=50,cp2+=50) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-52; k2+=52,bp2+=52*Bstride,ap2+=52*Astride) {
            _php_multnacc_fmfmf_mf_l1(ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            _php_multnacc_fmfmf_mf_l1_arb_k(K-k2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-52; k2+=52,bp2+=52*Bstride,ap2+=52*Astride) {
            _php_multnacc_fmfmf_mf_l1_arb_all(52,52,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            _php_multnacc_fmfmf_mf_l1_arb_all(52,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
   if (m2 < M) {
      c2 = C + m2*Cstride;
      a2 = A + m2;
      for (n2=0,b2=B,cp2=c2; n2<=N-50; n2+=50,b2+=50,cp2+=50) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-52; k2+=52,bp2+=52*Bstride,ap2+=52*Astride) {
            _php_multnacc_fmfmf_mf_l1_arb_all(M-m2,52,50,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            _php_multnacc_fmfmf_mf_l1_arb_all(M-m2,K-k2,50,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
      if (n2 < N) {
         for (k2=0,bp2=b2,ap2=a2; k2<=K-52; k2+=52,bp2+=52*Bstride,ap2+=52*Astride) {
            _php_multnacc_fmfmf_mf_l1_arb_all(M-m2,52,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
         if (k2 < K) {
            _php_multnacc_fmfmf_mf_l1_arb_all(M-m2,K-k2,N-n2,ap2,bp2,cp2,Astride,Bstride,Cstride,alpha);
         }
      }
   }
}

